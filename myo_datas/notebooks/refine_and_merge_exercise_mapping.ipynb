{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved refined mapping and merged dataset\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import json\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "BASE = Path('''/Users/corin44/Documents/myofitness/myo_datas''')\n",
        "PROCESSED = BASE / 'data' / 'processed'\n",
        "PROCESSED.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ex_path = PROCESSED / 'exercise_dataset_cleaned.csv'\n",
        "df_ex = pd.read_csv(ex_path, low_memory=False)\n",
        "\n",
        "# load existing mapping if present\n",
        "map_path = PROCESSED / 'exercises_canonical_mapping.json'\n",
        "if map_path.exists():\n",
        "    with open(map_path,'r') as f:\n",
        "        base_map = json.load(f)\n",
        "else:\n",
        "    base_map = {}\n",
        "\n",
        "# helper normalize\n",
        "STOPWORDS = set(['exercise','rep','reps','set','sets','kg','lb','lbs','bodyweight','with','and','the'])\n",
        "\n",
        "def normalize_name(s):\n",
        "    if pd.isna(s):\n",
        "        return ''\n",
        "    s = str(s).lower()\n",
        "    s = re.sub(r\"[^a-z0-9 ]+\", ' ', s)\n",
        "    tokens = [t.strip() for t in s.split() if t.strip() and t.strip() not in STOPWORDS]\n",
        "    tokens = [t for t in tokens if not t.isdigit()]\n",
        "    tokens = sorted(tokens)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# build candidates from column\n",
        "if 'Exercise_clean' in df_ex.columns:\n",
        "    raw_names = df_ex['Exercise_clean'].dropna().astype(str).unique().tolist()\n",
        "else:\n",
        "    raw_names = df_ex['Exercise'].dropna().astype(str).str.lower().str.replace(r\"[^a-z0-9 ]\", \"\", regex=True).str.strip().unique().tolist()\n",
        "\n",
        "# group by normalized token signature\n",
        "sig_to_names = defaultdict(list)\n",
        "for n in raw_names:\n",
        "    sig = normalize_name(n)\n",
        "    sig_to_names[sig].append(n)\n",
        "\n",
        "# build mapping, seed with existing\n",
        "canonical = dict(base_map)\n",
        "for sig, names in sig_to_names.items():\n",
        "    if not names:\n",
        "        continue\n",
        "    # choose canonical: most common in dataset (approx by counting occurrences)\n",
        "    counts = Counter(df_ex[df_ex['Exercise_clean'].isin(names)]['Exercise_clean']) if 'Exercise_clean' in df_ex.columns else Counter(df_ex[df_ex['Exercise'].str.lower().str.replace(r\"[^a-z0-9 ]\",\"\",regex=True).str.strip().isin(names)]['Exercise'])\n",
        "    if counts:\n",
        "        cand = counts.most_common(1)[0][0]\n",
        "    else:\n",
        "        cand = sorted(names, key=lambda s: (len(s.split()), len(s)))[0]\n",
        "    for n in names:\n",
        "        canonical[n] = cand\n",
        "\n",
        "# apply mapping to dataframe\n",
        "\n",
        "def map_to_canonical(x):\n",
        "    if pd.isna(x):\n",
        "        return x\n",
        "    key = str(x)\n",
        "    if key in canonical:\n",
        "        return canonical[key]\n",
        "    # fallback: normalized match\n",
        "    sig = normalize_name(key)\n",
        "    names = sig_to_names.get(sig, [])\n",
        "    if names:\n",
        "        return canonical.get(names[0], key)\n",
        "    return key\n",
        "\n",
        "if 'Exercise_clean' in df_ex.columns:\n",
        "    df_ex['Exercise_canonical'] = df_ex['Exercise_clean'].apply(map_to_canonical)\n",
        "else:\n",
        "    df_ex['Exercise_canonical'] = df_ex['Exercise'].apply(lambda s: map_to_canonical(str(s).lower().strip()))\n",
        "\n",
        "# save updated dataset\n",
        "out_ds = PROCESSED / 'exercise_dataset_canonical.csv'\n",
        "df_ex.to_csv(out_ds, index=False)\n",
        "\n",
        "# save mapping files\n",
        "with open(PROCESSED / 'exercises_canonical_mapping_refined.json','w') as f:\n",
        "    json.dump(canonical, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# preview top mappings\n",
        "items = list(canonical.items())[:500]\n",
        "preview_df = pd.DataFrame(items, columns=['raw','canonical'])\n",
        "preview_df.to_csv(PROCESSED / 'exercises_canonical_preview_refined.csv', index=False)\n",
        "\n",
        "print('Saved refined mapping and merged dataset')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
